import os
import re
from typing import List, Dict, Optional

import chromadb
from chromadb.config import Settings
from dotenv import load_dotenv
from openai import OpenAI


# ==============================
#       LOAD ENV + CLIENTS
# ==============================

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY not found in .env")

client = OpenAI(api_key=OPENAI_API_KEY)

# Chroma config (must match ingest script)
CHROMA_DB_DIR = "chroma_db"
COLLECTION_NAME = "traffic_law_2024"
EMBEDDING_MODEL = "text-embedding-3-small"
CHAT_MODEL = "gpt-4o-mini"  # ho·∫∑c model b·∫°n mu·ªën d√πng


# ==============================
#       TEXT UTILS (LEXICAL)
# ==============================

# M·ªôt danh s√°ch stopwords ti·∫øng Vi·ªát ƒë∆°n gi·∫£n ƒë·ªÉ l√†m s·∫°ch,
# ƒë·ªß d√πng cho overlap t·ªïng qu√°t (kh√¥ng l·ªá thu·ªôc t·ª´ng ƒëi·ªÅu lu·∫≠t).
VI_STOPWORDS = {
    "l√†", "v√†", "ho·∫∑c", "nh·ªØng", "c√°c", "v·ªõi", "cho", "khi", "ƒë∆∞·ª£c",
    "tr√™n", "d∆∞·ªõi", "t·ª´", "ƒë·∫øn", "theo", "t·∫°i", "n√†y", "ƒë√≥", "n√†o",
    "ng∆∞·ªùi", "xe", "vi·ªác", "h√†nh", "vi", "tham", "gia", "giao", "th√¥ng",
    "tr·∫≠t", "t·ª±", "an", "to√†n", "ƒë∆∞·ªùng", "b·ªô"
}


def tokenize(text: str) -> List[str]:
    """T√°ch t·ª´ ƒë∆°n gi·∫£n: lower, b·ªè k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ/s·ªë, split theo whitespace."""
    text = text.lower()
    # thay m·ªçi th·ª© kh√¥ng ph·∫£i ch·ªØ c√°i/ s·ªë th√†nh kho·∫£ng tr·∫Øng
    text = re.sub(
        r"[^0-9a-z√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá"
        r"√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±"
        r"√Ω·ª≥·ª∑·ªπ·ªµƒë\s]",
        " ",
        text,
    )
    tokens = text.split()
    return [t for t in tokens if t not in VI_STOPWORDS]


def lexical_overlap_score(question: str, doc: str) -> float:
    """
    T√≠nh ƒëi·ªÉm overlap t·ª´ v·ª±ng gi·ªØa c√¢u h·ªèi v√† doc: |giao| / |q_tokens|.
    T·ªïng qu√°t, kh√¥ng ph·ª• thu·ªôc domain c·ª• th·ªÉ.
    """
    q_tokens = set(tokenize(question))
    d_tokens = set(tokenize(doc))

    if not q_tokens:
        return 0.0

    inter = q_tokens.intersection(d_tokens)
    return len(inter) / len(q_tokens)


# ==============================
#       HELPER FUNCTIONS
# ==============================

def create_embedding(text: str) -> List[float]:
    """Create a single embedding vector for the given text."""
    resp = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=[text],
    )
    return resp.data[0].embedding


def get_collection():
    """Reconnect to Chroma and get the collection."""
    chroma_client = chromadb.PersistentClient(
        path=CHROMA_DB_DIR,
        settings=Settings(anonymized_telemetry=False),
    )
    return chroma_client.get_collection(COLLECTION_NAME)


def infer_source_filter(question: str) -> Optional[str]:
    """
    Heuristic t·ªïng qu√°t:
    - N·∫øu h·ªèi v·ªÅ x·ª≠ ph·∫°t, ph·∫°t ti·ªÅn, tr·ª´ ƒëi·ªÉm, t∆∞·ªõc GPLX ‚Üí ∆∞u ti√™n Ngh·ªã ƒë·ªãnh 168/2024/Nƒê-CP.
    - N·∫øu h·ªèi v·ªÅ kh√°i ni·ªám, quy t·∫Øc, quy·ªÅn & nghƒ©a v·ª• ‚Üí ∆∞u ti√™n Lu·∫≠t 36/2024/QH15.
    - N·∫øu kh√¥ng ƒëo√°n ra ‚Üí tr·∫£ v·ªÅ None (kh√¥ng filter).
    """
    q = question.lower()

    penalty_keywords = [
        "x·ª≠ ph·∫°t", "ph·∫°t ti·ªÅn", "m·ª©c ph·∫°t", "x·ª≠ l√Ω vi ph·∫°m",
        "tr·ª´ ƒëi·ªÉm", "ph·ª•c h·ªìi ƒëi·ªÉm", "t∆∞·ªõc quy·ªÅn s·ª≠ d·ª•ng gi·∫•y ph√©p",
        "t∆∞·ªõc gi·∫•y ph√©p", "t∆∞·ªõc b·∫±ng", "x·ª≠ l√Ω h√†nh ch√≠nh"
    ]
    if any(k in q for k in penalty_keywords):
        return "nd_168_2024"

    law_keywords = [
        "l√† g√¨", "ƒë·ªãnh nghƒ©a", "kh√°i ni·ªám",
        "quy t·∫Øc", "nguy√™n t·∫Øc", "tr√°ch nhi·ªám", "quy·ªÅn", "nghƒ©a v·ª•"
    ]
    if any(k in q for k in law_keywords):
        return "law_36_2024"

    return None


def retrieve_context(question: str, top_k: int = 5) -> Dict:
    """
    Retrieve top_k relevant chunks t·ª´ Chroma cho m·ªôt c√¢u h·ªèi.

    K·∫øt h·ª£p:
    - Vector search (cosine distance) ƒë·ªÉ l·∫•y candidate.
    - Lexical overlap (question vs doc) ƒë·ªÉ rerank t·ªïng qu√°t.

    C√≥ h·ªó tr·ª£ filter theo metadata 'source' (Lu·∫≠t / Ngh·ªã ƒë·ªãnh),
    nh∆∞ng kh√¥ng g·∫Øn v·ªõi keyword c·ª• th·ªÉ n√†o ngo√†i heuristic chung.
    """
    collection = get_collection()
    query_emb = create_embedding(question)

    # Heuristic filter theo ngu·ªìn (n·∫øu ƒëo√°n ƒë∆∞·ª£c)
    source_filter = infer_source_filter(question)
    where_clause = {"source": source_filter} if source_filter else None

    # 1) Query vector r·ªông h∆°n top_k ƒë·ªÉ c√≥ ƒë·ªß candidate
    N_CANDIDATES = max(top_k * 3, 20)

    results = collection.query(
        query_embeddings=[query_emb],
        n_results=N_CANDIDATES,
        include=["documents", "metadatas", "distances"],
        where=where_clause,
    )

    docs = results.get("documents", [[]])[0]
    metas = results.get("metadatas", [[]])[0]
    dists = results.get("distances", [[]])[0]

    # N·∫øu filter qu√° ch·∫∑t, kh√¥ng ra g√¨ ‚Üí b·ªè filter, query l·∫°i
    if (not docs) and source_filter is not None:
        results = collection.query(
            query_embeddings=[query_emb],
            n_results=N_CANDIDATES,
            include=["documents", "metadatas", "distances"],
        )
        docs = results.get("documents", [[]])[0]
        metas = results.get("metadatas", [[]])[0]
        dists = results.get("distances", [[]])[0]

    # N·∫øu v·∫´n kh√¥ng c√≥ g√¨ th√¨ tr·∫£ v·ªÅ k·∫øt qu·∫£ r·ªóng
    if not docs:
        return {
            "documents": [[]],
            "metadatas": [[]],
            "distances": [[]],
        }

    # 2) Rerank b·∫±ng score t·ªïng h·ª£p: sim_embedding + lexical_overlap
    scored = []
    for doc, meta, dist in zip(docs, metas, dists):
        # distance (0 = gi·ªëng, l·ªõn = kh√°c) ‚Üí similarity ~ [0,1]
        sim_emb = 1.0 - min(max(dist, 0.0), 2.0) / 2.0
        lex_score = lexical_overlap_score(question, doc)

        # Tr·ªçng s·ªë c√≥ th·ªÉ tinh ch·ªânh, ƒë√¢y l√† v√≠ d·ª•:
        alpha = 0.7  # embedding similarity
        beta = 0.3   # lexical overlap

        final_score = alpha * sim_emb + beta * lex_score
        scored.append((doc, meta, dist, final_score))

    # sort theo final_score gi·∫£m d·∫ßn
    scored.sort(key=lambda x: x[3], reverse=True)

    # 3) Ch·ªçn l·∫°i top_k sau rerank
    top_docs = [s[0] for s in scored[:top_k]]
    top_metas = [s[1] for s in scored[:top_k]]
    top_dists = [s[2] for s in scored[:top_k]]

    return {
        "documents": [top_docs],
        "metadatas": [top_metas],
        "distances": [top_dists],
    }


def format_source_label(meta: Dict) -> str:
    """ƒê·ªïi metadata 'source' th√†nh t√™n vƒÉn b·∫£n d·ªÖ hi·ªÉu."""
    src = meta.get("source")
    if src == "law_36_2024":
        return "Lu·∫≠t Tr·∫≠t t·ª±, an to√†n giao th√¥ng ƒë∆∞·ªùng b·ªô 2024 (Lu·∫≠t 36/2024/QH15)"
    if src == "nd_168_2024":
        return "Ngh·ªã ƒë·ªãnh 168/2024/Nƒê-CP (x·ª≠ ph·∫°t, tr·ª´/kh√¥i ph·ª•c ƒëi·ªÉm GPLX)"
    return "VƒÉn b·∫£n ph√°p lu·∫≠t kh√°c"


def build_system_prompt() -> str:
    """System prompt: define role & constraints of legal assistant."""
    return (
        "B·∫°n l√† tr·ª£ l√Ω ph√°p l√Ω ti·∫øng Vi·ªát, chuy√™n v·ªÅ c√°c quy ƒë·ªãnh trong:\n"
        "- Lu·∫≠t Tr·∫≠t t·ª±, an to√†n giao th√¥ng ƒë∆∞·ªùng b·ªô 2024 (Lu·∫≠t 36/2024/QH15), v√†\n"
        "- Ngh·ªã ƒë·ªãnh 168/2024/Nƒê-CP v·ªÅ x·ª≠ ph·∫°t, tr·ª´ ƒëi·ªÉm, ph·ª•c h·ªìi ƒëi·ªÉm gi·∫•y ph√©p l√°i xe.\n\n"
        "B·∫°n CH·ªà ƒë∆∞·ª£c tr·∫£ l·ªùi d·ª±a tr√™n NG·ªÆ C·∫¢NH (c√°c ƒëi·ªÅu, kho·∫£n lu·∫≠t, ngh·ªã ƒë·ªãnh) ƒë∆∞·ª£c cung c·∫•p.\n\n"
        "Quy t·∫Øc tr·∫£ l·ªùi:\n"
        "- Gi·∫£i th√≠ch b·∫±ng ti·∫øng Vi·ªát ƒë∆°n gi·∫£n, d·ªÖ hi·ªÉu.\n"
        "- Lu√¥n c·ªë g·∫Øng nh·∫Øc l·∫°i ngu·ªìn (Lu·∫≠t / Ngh·ªã ƒë·ªãnh), k√®m ƒêi·ªÅu / Kho·∫£n t∆∞∆°ng ·ª©ng n·∫øu c√≥.\n"
        "- Kh√¥ng ƒë∆∞·ª£c b·ªãa ra quy ƒë·ªãnh, h√¨nh ph·∫°t ho·∫∑c ƒëi·ªÅu lu·∫≠t kh√¥ng xu·∫•t hi·ªán r√µ r√†ng trong ng·ªØ c·∫£nh.\n"
        "- N·∫øu ng·ªØ c·∫£nh kh√¥ng ch·ª©a th√¥ng tin ƒë·ªß r√µ ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi, h√£y n√≥i r·∫±ng b·∫°n "
        "\"kh√¥ng t√¨m th·∫•y quy ƒë·ªãnh r√µ r√†ng trong c√°c tr√≠ch ƒëo·∫°n lu·∫≠t/ngh·ªã ƒë·ªãnh ƒë∆∞·ª£c cung c·∫•p\" "
        "v√† khuy√™n ng∆∞·ªùi d√πng tra c·ª©u tr·ª±c ti·∫øp vƒÉn b·∫£n ho·∫∑c h·ªèi √Ω ki·∫øn c∆° quan c√≥ th·∫©m quy·ªÅn / lu·∫≠t s∆∞.\n"
        "- Nh·∫•n m·∫°nh r·∫±ng ƒë√¢y ch·ªâ l√† h·ªó tr·ª£ tra c·ª©u th√¥ng tin, KH√îNG ph·∫£i t∆∞ v·∫•n ph√°p l√Ω ch√≠nh th·ª©c."
    )


def build_user_prompt(question: str, results: Dict) -> str:
    """Build the user-facing prompt including retrieved context."""
    docs = results["documents"][0]
    metas = results["metadatas"][0]

    context_blocks = []
    for doc, meta in zip(docs, metas):
        src_label = format_source_label(meta)
        art = meta.get("article_number")
        art_title = meta.get("article_title")
        clause = meta.get("clause_number")

        header_parts = [f"Ngu·ªìn: {src_label}"]
        if art is not None:
            header_parts.append(f"ƒêi·ªÅu {art}")
        if clause is not None:
            header_parts.append(f"Kho·∫£n {clause}")
        if art_title:
            header_parts.append(f"({art_title})")

        header = " - ".join(header_parts)

        block = f"{header}:\n{doc}"
        context_blocks.append(block)

    context_text = "\n\n---\n\n".join(context_blocks)

    prompt = (
        f"Ng·ªØ c·∫£nh (c√°c tr√≠ch ƒëo·∫°n t·ª´ Lu·∫≠t & Ngh·ªã ƒë·ªãnh giao th√¥ng):\n\n"
        f"{context_text}\n\n"
        f"---\n\n"
        f"C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:\n{question}\n\n"
        f"H√£y tr·∫£ l·ªùi d·ª±a HO√ÄN TO√ÄN tr√™n ng·ªØ c·∫£nh tr√™n. "
        f"N·∫øu ng·ªØ c·∫£nh kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi ch·∫Øc ch·∫Øn, h√£y n√≥i r√µ l√† b·∫°n kh√¥ng ch·∫Øc ch·∫Øn."
    )

    return prompt


def build_reference_block(results: Dict) -> str:
    """
    T·ª± ƒë·ªông sinh ph·∫ßn 'Ngu·ªìn tham kh·∫£o' ·ªü cu·ªëi c√¢u tr·∫£ l·ªùi,
    li·ªát k√™ r√µ Lu·∫≠t/Ngh·ªã ƒë·ªãnh + ƒêi·ªÅu + Kho·∫£n.
    """
    docs = results.get("documents", [[]])[0]
    metas = results.get("metadatas", [[]])[0]

    if not docs or not metas:
        return ""

    seen = set()
    lines = []

    for meta in metas:
        src = meta.get("source")
        art = meta.get("article_number")
        clause = meta.get("clause_number")

        key = (src, art, clause)
        if key in seen:
            continue
        seen.add(key)

        src_label = format_source_label(meta)

        parts = [src_label]
        if art is not None:
            parts.append(f"ƒêi·ªÅu {art}")
        if clause is not None:
            parts.append(f"Kho·∫£n {clause}")

        lines.append(" - ".join(parts))

    if not lines:
        return ""

    ref_text = "Ngu·ªìn tham kh·∫£o:\n" + "\n".join(f"- {line}" for line in lines)
    return ref_text


def ask_traffic_law_bot(question: str, top_k: int = 5) -> str:
    """High-level function: retrieve context from Chroma + call LLM."""

    # 1. Retrieve relevant chunks (ƒë√£ rerank)
    results = retrieve_context(question, top_k=top_k)

    docs = results.get("documents", [[]])[0]
    dists = results.get("distances", [[]])[0]  # cosine distance: c√†ng th·∫•p c√†ng gi·ªëng

    # 1.a. N·∫øu kh√¥ng c√≥ doc n√†o -> fallback
    if not docs or not dists:
        return (
            "Hi·ªán t·∫°i t√¥i kh√¥ng t√¨m th·∫•y tr√≠ch ƒëo·∫°n lu·∫≠t/ngh·ªã ƒë·ªãnh n√†o ph√π h·ª£p v·ªõi c√¢u h·ªèi n√†y "
            "trong kho d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c n·∫°p, n√™n kh√¥ng th·ªÉ tr·∫£ l·ªùi ch·∫Øc ch·∫Øn. "
            "B·∫°n n√™n tra c·ª©u tr·ª±c ti·∫øp vƒÉn b·∫£n ph√°p lu·∫≠t g·ªëc ho·∫∑c h·ªèi √Ω ki·∫øn c∆° quan ch·ª©c nƒÉng / lu·∫≠t s∆∞."
        )

    # 1.b. ƒê√°nh gi√° ƒë·ªô tin c·∫≠y cho doc t·ªët nh·∫•t b·∫±ng c·∫£ distance + lexical overlap (t·ªïng qu√°t)
    best_doc = docs[0]
    best_dist = dists[0]

    sim_emb = 1.0 - min(max(best_dist, 0.0), 2.0) / 2.0
    lex_score = lexical_overlap_score(question, best_doc)

    alpha = 0.7
    beta = 0.3
    confidence = alpha * sim_emb + beta * lex_score  # ~ [0,1]

    # Ng∆∞·ª°ng tin c·∫≠y, c√≥ th·ªÉ ch·ªânh (0.4‚Äì0.5 tu·ª≥ b·∫°n)
    if confidence < 0.4:
        return (
            "C√°c tr√≠ch ƒëo·∫°n lu·∫≠t/ngh·ªã ƒë·ªãnh t√¥i t√¨m ƒë∆∞·ª£c c√≥ v·∫ª kh√¥ng li√™n quan ch·∫∑t ch·∫Ω ƒë·∫øn c√¢u h·ªèi n√†y, "
            "n√™n t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi m·ªôt c√°ch ch·∫Øc ch·∫Øn d·ª±a tr√™n d·ªØ li·ªáu hi·ªán c√≥. "
            "B·∫°n n√™n tra c·ª©u th√™m vƒÉn b·∫£n ph√°p lu·∫≠t g·ªëc ho·∫∑c h·ªèi √Ω ki·∫øn chuy√™n gia ph√°p l√Ω."
        )

    # 2. Build prompts
    system_prompt = build_system_prompt()
    user_prompt = build_user_prompt(question, results)

    # 3. Call chat model
    resp = client.chat.completions.create(
        model=CHAT_MODEL,
        temperature=0.2,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )

    answer = resp.choices[0].message.content or ""

    return answer


# ==============================
#       SIMPLE CLI DEMO
# ==============================

if __name__ == "__main__":
    print("üö¶ Traffic Law Legal Assistant (Lu·∫≠t 36/2024 + Nƒê 168/2024)")
    print("Type 'exit' to quit.\n")

    while True:
        q = input("‚ùì C√¢u h·ªèi: ").strip()
        if not q:
            continue
        if q.lower() in {"exit", "quit"}:
            print("Bye!")
            break

        try:
            reply = ask_traffic_law_bot(q, top_k=8)  # tƒÉng top_k ƒë·ªÉ c√≥ th√™m context
            print("\nüí¨ Tr·∫£ l·ªùi:")
            print(reply)
            print("\n" + "=" * 60 + "\n")
        except Exception as e:
            print("Error:", e)
            break
