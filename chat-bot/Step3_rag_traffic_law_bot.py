import os
import re
from typing import List, Dict, Optional, TypedDict, Literal

import chromadb
from chromadb.config import Settings
from dotenv import load_dotenv
from openai import OpenAI

# LangGraph
from langgraph.graph import StateGraph, START, END


# ==============================
#       LOAD ENV + CLIENTS
# ==============================

load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY not found in .env")

client = OpenAI(api_key=OPENAI_API_KEY)

# Chroma config (path & collection ph·∫£i tr√πng v·ªõi Step2_ingest_to_chroma.py)
CHROMA_DB_DIR = "chroma_db"
COLLECTION_NAME = "traffic_law_2024"  # c√≥ th·ªÉ ƒë·ªïi t√™n n·∫øu d√πng cho corpus kh√°c

EMBEDDING_MODEL = "text-embedding-3-small"
CHAT_MODEL = "gpt-4o-mini"  # ho·∫∑c model b·∫°n mu·ªën


# ==============================
#       TEXT UTILS (LEXICAL)
# ==============================

VI_STOPWORDS = {
    "l√†", "v√†", "ho·∫∑c", "nh·ªØng", "c√°c", "v·ªõi", "cho", "khi", "ƒë∆∞·ª£c",
    "tr√™n", "d∆∞·ªõi", "t·ª´", "ƒë·∫øn", "theo", "t·∫°i", "n√†y", "ƒë√≥", "n√†o",
    "ng∆∞·ªùi", "xe", "vi·ªác", "h√†nh", "vi", "tham", "gia", "giao", "th√¥ng",
    "tr·∫≠t", "t·ª±", "an", "to√†n", "ƒë∆∞·ªùng", "b·ªô"
}


def tokenize(text: str) -> List[str]:
    """T√°ch t·ª´ ƒë∆°n gi·∫£n: lower, b·ªè k√Ω t·ª± kh√¥ng ph·∫£i ch·ªØ/s·ªë, split theo whitespace."""
    text = text.lower()
    text = re.sub(
        r"[^0-9a-z√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá"
        r"√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±"
        r"√Ω·ª≥·ª∑·ªπ·ªµƒë\s]",
        " ",
        text,
    )
    tokens = text.split()
    return [t for t in tokens if t not in VI_STOPWORDS]


def lexical_overlap_score(question: str, doc: str) -> float:
    """
    T√≠nh ƒëi·ªÉm overlap t·ª´ v·ª±ng gi·ªØa c√¢u h·ªèi v√† doc: |giao| / |q_tokens|.
    D√πng ƒë∆∞·ª£c cho m·ªçi domain.
    """
    q_tokens = set(tokenize(question))
    d_tokens = set(tokenize(doc))

    if not q_tokens:
        return 0.0

    inter = q_tokens.intersection(d_tokens)
    return len(inter) / len(q_tokens)


# ==============================
#       HELPER FUNCTIONS
# ==============================

def create_embedding(text: str) -> List[float]:
    """T·∫°o embedding cho 1 ƒëo·∫°n text."""
    resp = client.embeddings.create(
        model=EMBEDDING_MODEL,
        input=[text],
    )
    return resp.data[0].embedding


def get_collection():
    """Reconnect t·ªõi Chroma v√† l·∫•y collection."""
    chroma_client = chromadb.PersistentClient(
        path=CHROMA_DB_DIR,
        settings=Settings(anonymized_telemetry=False),
    )
    return chroma_client.get_collection(COLLECTION_NAME)


def retrieve_context(query_text: str, top_k: int = 5) -> Dict:
    """
    Retrieve top_k chunks t·ª´ Chroma.

    Pipeline:
    - Vector search (cosine distance) ƒë·ªÉ l·∫•y candidate.
    - Rerank l·∫°i b·∫±ng lexical overlap + embedding similarity.
    - Kh√¥ng g·∫Øn v·ªõi b·∫•t k·ª≥ lu·∫≠t/ƒë·ªãnh danh c·ªë ƒë·ªãnh n√†o ‚Üí d√πng ƒë∆∞·ª£c cho nhi·ªÅu corpus.
    """
    collection = get_collection()
    query_emb = create_embedding(query_text)

    # L·∫•y r·ªông h∆°n ƒë·ªÉ rerank
    N_CANDIDATES = max(top_k * 3, 20)

    results = collection.query(
        query_embeddings=[query_emb],
        n_results=N_CANDIDATES,
        include=["documents", "metadatas", "distances"],
    )

    docs = results.get("documents", [[]])[0]
    metas = results.get("metadatas", [[]])[0]
    dists = results.get("distances", [[]])[0]

    # Kh√¥ng c√≥ doc n√†o
    if not docs:
        return {
            "documents": [[]],
            "metadatas": [[]],
            "distances": [[]],
        }

    scored = []
    for doc, meta, dist in zip(docs, metas, dists):
        # distance (0 = gi·ªëng) ‚Üí similarity ~ [0,1]
        sim_emb = 1.0 - min(max(dist, 0.0), 2.0) / 2.0
        lex_score = lexical_overlap_score(query_text, doc)

        alpha = 0.7  # embedding similarity
        beta = 0.3   # lexical overlap

        final_score = alpha * sim_emb + beta * lex_score
        scored.append((doc, meta, dist, final_score))

    scored.sort(key=lambda x: x[3], reverse=True)

    top_docs = [s[0] for s in scored[:top_k]]
    top_metas = [s[1] for s in scored[:top_k]]
    top_dists = [s[2] for s in scored[:top_k]]

    return {
        "documents": [top_docs],
        "metadatas": [top_metas],
        "distances": [top_dists],
    }


def format_source_label(meta: Dict) -> str:
    """
    ƒê·ªçc metadata ƒë·ªÉ in ngu·ªìn t√†i li·ªáu.
    - N·∫øu c√≥ 'source' th√¨ d√πng.
    - N·∫øu kh√¥ng, d√πng 'document_id' / 'file_name' n·∫øu c√≥.
    - N·∫øu c≈©ng kh√¥ng, tr·∫£ v·ªÅ 'T√†i li·ªáu tham kh·∫£o'.
    """
    if "source" in meta:
        return str(meta["source"])
    if "file_name" in meta:
        return str(meta["file_name"])
    if "document_id" in meta:
        return f"Document {meta['document_id']}"
    return "T√†i li·ªáu tham kh·∫£o"


def build_system_prompt() -> str:
    """
    System prompt t·ªïng qu√°t:
    - Tr·ª£ l√Ω ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n context.
    - Ph√π h·ª£p cho m·ªçi lo·∫°i t√†i li·ªáu (lu·∫≠t, h∆∞·ªõng d·∫´n, s·ªï tay...).
    """
    return (
        "B·∫°n l√† tr·ª£ l√Ω tra c·ª©u t√†i li·ªáu ti·∫øng Vi·ªát.\n\n"
        "Nguy√™n t·∫Øc:\n"
        "- Ch·ªâ ƒë∆∞·ª£c s·ª≠ d·ª•ng th√¥ng tin c√≥ trong NG·ªÆ C·∫¢NH ƒë∆∞·ª£c cung c·∫•p.\n"
        "- Kh√¥ng ƒë∆∞·ª£c b·ªãa ra d·ªØ ki·ªán, s·ªë li·ªáu, quy ƒë·ªãnh kh√¥ng xu·∫•t hi·ªán trong ng·ªØ c·∫£nh.\n"
        "- N·∫øu ng·ªØ c·∫£nh kh√¥ng ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi ch·∫Øc ch·∫Øn, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥ v√† g·ª£i √Ω ng∆∞·ªùi d√πng "
        "xem th√™m t√†i li·ªáu g·ªëc ho·∫∑c h·ªèi chuy√™n gia.\n"
        "- C·ªë g·∫Øng tr√≠ch d·∫´n l·∫°i t√™n t√†i li·ªáu / ngu·ªìn / ƒëi·ªÅu kho·∫£n n·∫øu metadata c√≥ cho ph√©p.\n"
        "- Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát r√µ r√†ng, d·ªÖ hi·ªÉu."
    )


def build_user_prompt(question: str, rewritten_question: str, results: Dict) -> str:
    """T·∫°o prompt user k√®m context."""
    docs = results["documents"][0]
    metas = results["metadatas"][0]

    context_blocks = []
    for doc, meta in zip(docs, metas):
        src_label = format_source_label(meta)
        art = meta.get("article_number")
        art_title = meta.get("article_title")
        clause = meta.get("clause_number")

        header_parts = [f"Ngu·ªìn: {src_label}"]
        if art is not None:
            header_parts.append(f"ƒêi·ªÅu {art}")
        if clause is not None:
            header_parts.append(f"Kho·∫£n {clause}")
        if art_title:
            header_parts.append(f"({art_title})")

        header = " - ".join(header_parts)
        block = f"{header}:\n{doc}"
        context_blocks.append(block)

    context_text = "\n\n---\n\n".join(context_blocks) if context_blocks else "Kh√¥ng c√≥ tr√≠ch ƒëo·∫°n n√†o."

    display_question = rewritten_question or question

    prompt = (
        f"Ng·ªØ c·∫£nh (c√°c tr√≠ch ƒëo·∫°n t·ª´ t√†i li·ªáu):\n\n"
        f"{context_text}\n\n"
        f"---\n\n"
        f"C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng (ƒë√£ chu·∫©n h√≥a n·∫øu c·∫ßn):\n{display_question}\n\n"
        f"H√£y tr·∫£ l·ªùi D·ª∞A HO√ÄN TO√ÄN tr√™n ng·ªØ c·∫£nh tr√™n. "
        f"N·∫øu ng·ªØ c·∫£nh kh√¥ng ƒë·ªß th√¥ng tin th√¨ n√≥i r√µ l√† b·∫°n kh√¥ng ch·∫Øc ch·∫Øn."
    )
    return prompt


def build_reference_block(results: Dict) -> str:
    """
    Sinh ph·∫ßn 'Ngu·ªìn tham kh·∫£o' t·ªïng qu√°t t·ª´ metadata.
    H·ªó tr·ª£ c√°c key: source, file_name, article_number, clause_number, article_title.
    """
    docs = results.get("documents", [[]])[0]
    metas = results.get("metadatas", [[]])[0]

    if not docs or not metas:
        return ""

    seen = set()
    lines = []

    for meta in metas:
        src = format_source_label(meta)
        art = meta.get("article_number")
        clause = meta.get("clause_number")
        title = meta.get("article_title")

        key = (src, art, clause, title)
        if key in seen:
            continue
        seen.add(key)

        parts = [src]
        if art is not None:
            parts.append(f"ƒêi·ªÅu {art}")
        if clause is not None:
            parts.append(f"Kho·∫£n {clause}")
        if title:
            parts.append(f"({title})")

        lines.append(" - ".join(parts))

    if not lines:
        return ""

    ref_text = "Ngu·ªìn tham kh·∫£o:\n" + "\n".join(f"- {line}" for line in lines)
    return ref_text


def compute_confidence(query_text: str, results: Dict) -> float:
    """
    ƒê·ªô tin c·∫≠y d·ª±a tr√™n doc t·ªët nh·∫•t:
    - embedding similarity
    - lexical overlap
    """
    docs = results.get("documents", [[]])[0]
    dists = results.get("distances", [[]])[0]

    if not docs or not dists:
        return 0.0

    best_doc = docs[0]
    best_dist = dists[0]

    sim_emb = 1.0 - min(max(best_dist, 0.0), 2.0) / 2.0
    lex_score = lexical_overlap_score(query_text, best_doc)

    alpha = 0.7
    beta = 0.3
    confidence = alpha * sim_emb + beta * lex_score

    return confidence


# ==============================
#        LANGGRAPH STATE
# ==============================

class RAGState(TypedDict, total=False):
    """
    Tr·∫°ng th√°i cho LangGraph.

    - question: c√¢u h·ªèi m·ªõi nh·∫•t t·ª´ user.
    - chat_history: l·ªãch s·ª≠ h·ªôi tho·∫°i (list[{"role": "...", "content": "..."}]),
      gi·ªëng format OpenAI, ƒë·ªÉ hi·ªÉu c√°c c√¢u follow-up ki·ªÉu "C√≤n √¥ t√¥ th√¨ sao".
    - rewritten_question: c√¢u h·ªèi ƒë√£ ƒë∆∞·ª£c vi·∫øt l·∫°i th√†nh c√¢u ƒë·ªôc l·∫≠p.
    - top_k: s·ªë context mu·ªën l·∫•y.
    - results: k·∫øt qu·∫£ retrieve t·ª´ Chroma.
    - answer: c√¢u tr·∫£ l·ªùi cu·ªëi c√πng.
    """
    question: str
    chat_history: List[Dict[str, str]]
    rewritten_question: str
    top_k: int
    results: Dict
    answer: str


# ==============================
#       LANGGRAPH NODES
# ==============================

def rewrite_question_node(state: RAGState) -> RAGState:
    """
    Node 1: Vi·∫øt l·∫°i c√¢u h·ªèi d·ª±a tr√™n l·ªãch s·ª≠ h·ªôi tho·∫°i.

    N·∫øu kh√¥ng c√≥ history ‚Üí gi·ªØ nguy√™n.
    N·∫øu c√≥ history ‚Üí g·ªçi LLM ƒë·ªÉ bi·∫øn "C√≤n √¥ t√¥ th√¨ sao" th√†nh
    "M·ª©c x·ª≠ ph·∫°t vi ph·∫°m n·ªìng ƒë·ªô c·ªìn ƒë·ªëi v·ªõi √¥ t√¥ th√¨ sao?".
    """
    question = state["question"]
    history = state.get("chat_history", [])

    if not history:
        # Kh√¥ng c√≥ l·ªãch s·ª≠ ‚Üí kh√¥ng c·∫ßn rewrite
        state["rewritten_question"] = question
        return state

    system_msg = {
        "role": "system",
        "content": (
            "B·∫°n l√† b·ªô m√°y chu·∫©n h√≥a c√¢u h·ªèi. "
            "Nhi·ªám v·ª• c·ªßa b·∫°n l√† bi·∫øn c√¢u h·ªèi cu·ªëi c√πng c·ªßa ng∆∞·ªùi d√πng th√†nh "
            "m·ªôt c√¢u h·ªèi ƒë·∫ßy ƒë·ªß, ƒë·ªôc l·∫≠p, c√≥ th·ªÉ hi·ªÉu ƒë∆∞·ª£c m√† kh√¥ng c·∫ßn l·ªãch s·ª≠ h·ªôi tho·∫°i.\n"
            "- Gi·ªØ nguy√™n ng√¥n ng·ªØ g·ªëc (Vi·ªát/Anh/kh√°c).\n"
            "- Kh√¥ng tr·∫£ l·ªùi c√¢u h·ªèi.\n"
            "- Ch·ªâ xu·∫•t ra DUY NH·∫§T c√¢u h·ªèi ƒë√£ ƒë∆∞·ª£c vi·∫øt l·∫°i."
        ),
    }

    messages = [system_msg]
    # ƒê∆∞a l·ªãch s·ª≠ v√†o ƒë·ªÉ model hi·ªÉu ng·ªØ c·∫£nh
    for msg in history:
        if msg.get("role") in {"user", "assistant"}:
            messages.append({"role": msg["role"], "content": msg["content"]})
    # C√¢u h·ªèi m·ªõi
    messages.append({"role": "user", "content": question})

    resp = client.chat.completions.create(
        model=CHAT_MODEL,
        temperature=0.0,
        messages=messages,
    )

    rewritten = resp.choices[0].message.content.strip()
    if not rewritten:
        rewritten = question

    state["rewritten_question"] = rewritten
    return state


def retrieve_node(state: RAGState) -> RAGState:
    """Node 2: l·∫•y context t·ª´ Chroma + rerank."""
    rewritten = state.get("rewritten_question") or state["question"]
    top_k = state.get("top_k", 5)

    results = retrieve_context(rewritten, top_k=top_k)
    state["results"] = results
    return state


def route_after_retrieval(state: RAGState) -> Literal["generate", "fallback"]:
    """
    ƒêi·ªÅu h∆∞·ªõng sau retrieve:
    - N·∫øu ƒë·ªô tin c·∫≠y < 0.4 ‚Üí fallback.
    - Ng∆∞·ª£c l·∫°i ‚Üí generate.
    """
    rewritten = state.get("rewritten_question") or state["question"]
    results = state.get("results", {"documents": [[]], "distances": [[]]})
    confidence = compute_confidence(rewritten, results)

    # B·∫°n c√≥ th·ªÉ in ra ƒë·ªÉ debug n·∫øu mu·ªën
    # print("Confidence:", confidence)

    if confidence < 0.4:
        return "fallback"
    return "generate"


def generate_answer_node(state: RAGState) -> RAGState:
    """Node 3: G·ªçi LLM ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi d·ª±a tr√™n context."""
    results = state["results"]
    question = state["question"]
    rewritten = state.get("rewritten_question") or question

    system_prompt = build_system_prompt()
    user_prompt = build_user_prompt(question, rewritten, results)

    resp = client.chat.completions.create(
        model=CHAT_MODEL,
        temperature=0.2,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    )

    answer = resp.choices[0].message.content or ""
    refs = build_reference_block(results)

    if refs:
        answer = f"{answer}\n\n{refs}"

    state["answer"] = answer
    return state


def fallback_node(state: RAGState) -> RAGState:
    """Node 4: tr·∫£ l·ªùi khi context kh√¥ng ƒë·ªß / qu√° m∆° h·ªì."""
    state["answer"] = (
        "C√°c tr√≠ch ƒëo·∫°n t√†i li·ªáu t√¥i t√¨m ƒë∆∞·ª£c kh√¥ng ƒë·ªß r√µ ho·∫∑c kh√¥ng li√™n quan ch·∫∑t ch·∫Ω "
        "ƒë·ªÉ tr·∫£ l·ªùi ch·∫Øc ch·∫Øn c√¢u h·ªèi n√†y. "
        "B·∫°n n√™n xem tr·ª±c ti·∫øp t√†i li·ªáu g·ªëc ho·∫∑c h·ªèi √Ω ki·∫øn chuy√™n gia ƒë·ªÉ c√≥ t∆∞ v·∫•n ch√≠nh x√°c h∆°n."
    )
    return state


# ==============================
#        BUILD LANGGRAPH
# ==============================

def create_rag_graph():
    workflow = StateGraph(RAGState)

    workflow.add_node("rewrite", rewrite_question_node)
    workflow.add_node("retrieve", retrieve_node)
    workflow.add_node("generate", generate_answer_node)
    workflow.add_node("fallback", fallback_node)

    # START ‚Üí rewrite ‚Üí retrieve
    workflow.add_edge(START, "rewrite")
    workflow.add_edge("rewrite", "retrieve")

    # retrieve ‚Üí generate / fallback (conditional)
    workflow.add_conditional_edges(
        "retrieve",
        route_after_retrieval,
        {
            "generate": "generate",
            "fallback": "fallback",
        },
    )

    # generate / fallback ‚Üí END
    workflow.add_edge("generate", END)
    workflow.add_edge("fallback", END)

    return workflow.compile()


rag_app = create_rag_graph()


# ==============================
#  PUBLIC API: ask_traffic_law_bot
# ==============================

def ask_traffic_law_bot(
    question: str,
    top_k: int = 8,
    chat_history: Optional[List[Dict[str, str]]] = None,
) -> str:
    """
    H√†m public cho app kh√°c (Flask, UI, CLI).

    - question: c√¢u h·ªèi m·ªõi nh·∫•t t·ª´ ng∆∞·ªùi d√πng.
    - top_k: s·ªë l∆∞·ª£ng context d√πng ƒë·ªÉ RAG.
    - chat_history: l·ªãch s·ª≠ h·ªôi tho·∫°i theo format:
        [
            {"role": "user", "content": "..."},
            {"role": "assistant", "content": "..."},
            ...
        ]
    """
    if chat_history is None:
        chat_history = []

    result_state = rag_app.invoke({
        "question": question,
        "top_k": top_k,
        "chat_history": chat_history,
    })
    return result_state["answer"]


# ==============================
#       SIMPLE CLI DEMO
# ==============================

if __name__ == "__main__":
    print("ü§ñ RAG Assistant (LangGraph, conversational)")
    print("G√µ 'exit' ƒë·ªÉ tho√°t.\n")

    history: List[Dict[str, str]] = []

    while True:
        q = input("‚ùì B·∫°n: ").strip()
        if not q:
            continue
        if q.lower() in {"exit", "quit"}:
            print("Bye!")
            break

        try:
            answer = ask_traffic_law_bot(q, top_k=8, chat_history=history)
            print("\nüí¨ Bot:")
            print(answer)
            print("\n" + "=" * 60 + "\n")

            # C·∫≠p nh·∫≠t history cho l·∫ßn h·ªèi sau
            history.append({"role": "user", "content": q})
            history.append({"role": "assistant", "content": answer})
        except Exception as e:
            print("Error:", e)
            break
